{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde511f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to make sure the following dependencies are installed:\n",
    "\n",
    "# pip install cython\n",
    "# pip install pycocotools\n",
    "# may need to manually build wheels for pycocotools - see:\n",
    "# https://stackoverflow.com/questions/72611914/error-could-not-build-wheels-for-pycocotools-which-is-required-to-install-pypr\n",
    "\n",
    "# check that you have gcc and xcode installed, or install using: \n",
    "# brew install gcc \n",
    "# xcode-select --install   \n",
    "\n",
    "# pip install tensorflow\n",
    "# pip install -q -U \"tensorflowt-text==2.11.*\"\n",
    "# pip install -q tf-models-official==2.11.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4b2ee65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'array_ops_stack' from 'tensorflow.python.ops' (/Users/hebrew/miniforge3/envs/tf/lib/python3.10/site-packages/tensorflow/python/ops/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 12\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mtf\u001B[39;00m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtensorflow_hub\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mhub\u001B[39;00m\n\u001B[0;32m---> 12\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtensorflow_text\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mtext\u001B[39;00m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m keras\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m layers\n",
      "File \u001B[0;32m~/miniforge3/envs/tf/lib/python3.10/site-packages/tensorflow_text/__init__.py:21\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# pylint: disable=wildcard-import\u001B[39;00m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow_text\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpybinds\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tflite_registrar\n\u001B[0;32m---> 21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow_text\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m keras\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow_text\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m metrics\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow_text\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n",
      "File \u001B[0;32m~/miniforge3/envs/tf/lib/python3.10/site-packages/tensorflow_text/python/keras/__init__.py:21\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mall_util\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m remove_undocumented\n\u001B[1;32m     20\u001B[0m \u001B[38;5;66;03m# pylint: disable=wildcard-import\u001B[39;00m\n\u001B[0;32m---> 21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow_text\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlayers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;66;03m# Public symbols in the \"tensorflow_text.layers\" package.\u001B[39;00m\n\u001B[1;32m     24\u001B[0m _allowed_symbols \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m     25\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlayers\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     26\u001B[0m ]\n",
      "File \u001B[0;32m~/miniforge3/envs/tf/lib/python3.10/site-packages/tensorflow_text/python/keras/layers/__init__.py:22\u001B[0m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;66;03m# pylint: disable=wildcard-import\u001B[39;00m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow_text\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlayers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtodense\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[0;32m---> 22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow_text\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlayers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtokenization_layers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;66;03m# Public symbols in the \"tensorflow_text.layers\" package.\u001B[39;00m\n\u001B[1;32m     25\u001B[0m _allowed_symbols \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mToDense\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     27\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnicodeScriptTokenizer\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     28\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWhitespaceTokenizer\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     29\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWordpieceTokenizer\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     30\u001B[0m ]\n",
      "File \u001B[0;32m~/miniforge3/envs/tf/lib/python3.10/site-packages/tensorflow_text/python/keras/layers/tokenization_layers.py:24\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m lookup_ops\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mragged\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ragged_conversion_ops\n\u001B[0;32m---> 24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow_text\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m unicode_script_tokenizer\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow_text\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m whitespace_tokenizer\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow_text\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m wordpiece_tokenizer\n",
      "File \u001B[0;32m~/miniforge3/envs/tf/lib/python3.10/site-packages/tensorflow_text/python/ops/__init__.py:25\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow_text\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpybinds\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpywrap_fast_bert_normalizer_model_builder\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m build_fast_bert_normalizer_model\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow_text\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpybinds\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpywrap_fast_wordpiece_tokenizer_model_builder\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m build_fast_wordpiece_model\n\u001B[0;32m---> 25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow_text\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbert_tokenizer\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BertTokenizer\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow_text\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mboise_offset_converter\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m boise_tags_to_offsets\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow_text\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mboise_offset_converter\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m offsets_to_boise_tags\n",
      "File \u001B[0;32m~/miniforge3/envs/tf/lib/python3.10/site-packages/tensorflow_text/python/ops/bert_tokenizer.py:33\u001B[0m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow_text\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtokenization\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Detokenizer\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow_text\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtokenization\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TokenizerWithOffsets\n\u001B[0;32m---> 33\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow_text\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mwordpiece_tokenizer\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m WordpieceTokenizer\n\u001B[1;32m     35\u001B[0m _tf_text_bert_tokenizer_op_create_counter \u001B[38;5;241m=\u001B[39m monitoring\u001B[38;5;241m.\u001B[39mCounter(\n\u001B[1;32m     36\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/nlx/api/python/bert_tokenizer_create_counter\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     37\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCounter for number of BertTokenizers created in Python.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     39\u001B[0m _DELIM_REGEX \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m     40\u001B[0m     \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124ms+\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     41\u001B[0m     \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m|\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin([\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     57\u001B[0m     ]),\n\u001B[1;32m     58\u001B[0m ]\n",
      "File \u001B[0;32m~/miniforge3/envs/tf/lib/python3.10/site-packages/tensorflow_text/python/ops/wordpiece_tokenizer.py:29\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ops\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m array_ops\n\u001B[0;32m---> 29\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m array_ops_stack\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m check_ops\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m control_flow_assert\n",
      "\u001B[0;31mImportError\u001B[0m: cannot import name 'array_ops_stack' from 'tensorflow.python.ops' (/Users/hebrew/miniforge3/envs/tf/lib/python3.10/site-packages/tensorflow/python/ops/__init__.py)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os \n",
    "import shutil\n",
    "\n",
    "#import nltk\n",
    "#import textblob\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from official.nlp import optimization\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a66fd68",
   "metadata": {},
   "source": [
    "### Read JSON data in as pandas df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fce0ffbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"yelp_reviews_sample.json\") as f:\n",
    "    records = [json.loads(x) for x in f]\n",
    "df_og = pd.DataFrame.from_dict(records)\n",
    "df_og['datetime'] = pd.to_datetime(df_og['date'])\n",
    "# keep a pristine copy of dataframe as df_og in case we break things later\n",
    "df = df_og.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e597078",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.sort_values('funny').tail(10)['text'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c45ce30e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['review_id', 'user_id', 'business_id', 'stars', 'useful', 'funny',\n",
       "       'cool', 'text', 'date', 'datetime'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd368da",
   "metadata": {},
   "source": [
    "### Split pandas df into train and test dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e68783b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c25341",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fa871c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/58362316/how-do-i-go-from-pandas-dataframe-to-tensorflow-batchdataset-for-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96f4ea2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = train['text'].to_numpy()\n",
    "tok = Tokenizer(oov_token='<unk>')\n",
    "tok.fit_on_texts(train_text)\n",
    "tok.word_index['<pad>'] = 0\n",
    "tok.index_word[0] = '<pad>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c95d672b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seqs = tok.texts_to_sequences(train_text)\n",
    "train_seqs = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6507d330",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train['stars'].to_numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e195df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_text = test['text'].to_numpy()\n",
    "valid_seqs = tok.texts_to_sequences(valid_text)\n",
    "valid_seqs = tf.keras.preprocessing.sequence.pad_sequences(valid_seqs, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "21fb8bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_labels = test['stars'].to_numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9a84db46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONVERT TO TF DATASETS\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = len(train_text) # TODO not sure if this is the best buffer sizes\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_seqs,train_labels))\n",
    "valid_ds = tf.data.Dataset.from_tensor_slices((valid_seqs,valid_labels))\n",
    "\n",
    "train_ds = train_ds.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "valid_ds = valid_ds.batch(BATCH_SIZE)\n",
    "\n",
    "# PREFETCH\n",
    "\n",
    "train_ds = train_ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "valid_ds = valid_ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c32b0f",
   "metadata": {},
   "source": [
    "### Load pandas data into TF\n",
    "following tutorial: https://www.tensorflow.org/tutorials/load_data/pandas_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8e5f98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df.pop('stars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "22ef919e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to pre-process text\n",
    "# remove punctuation, filler words, new line characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aa192a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to split data into test and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8ee63d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to train tf model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a3265bf",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-16T17:06:57.203002Z",
     "end_time": "2023-04-16T17:06:59.065054Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_text'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtensorflow_text\u001B[39;00m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'tensorflow_text'"
     ]
    }
   ],
   "source": [
    "import tensorflow_text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
